{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [scikit-learn](https://scikit-learn.org/stable/user_guide.html) 入門\n",
    "---\n",
    "\n",
    "+ オープンソースの機械学習ライブラリ\n",
    "+ (2007年) Google Summer of Code projectとしてDavid Cournapeau氏によって開発\n",
    "+ Numpy、SciPy、matplotlibといったPythonのライブラリ上で動作\n",
    "+ 手軽に機械学習の 前処理、 モデリング、 評価指標算出 \n",
    "+ アルゴリズムが豊富\n",
    "> ![img](https://www.codexa.net/wp-content/uploads/2020/09/スクリーンショット-2020-09-29-11.20.30.png)\n",
    "> 公式には使用マップがある！(アルゴリズムチートシート)\n",
    "\n",
    "cf. [scikit-learn 入門：6つの機能と分類・回帰の実装方法を徹底解説！](https://www.codexa.net/scikit-learn-intro/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn \n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris, load_boston\n",
    "# アヤメ(iris)のデータセットと、ボストンの住宅価格のデータセットをインポート"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分類\n",
    "> sepal:がく petal:花弁（花びら）\n",
    ">\n",
    "> がくと花弁それぞれの長さと幅から、アヤメの種類を当てる分類問題を解くためのデータセット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "0                5.1               3.5                1.4               0.2\n",
       "1                4.9               3.0                1.4               0.2\n",
       "2                4.7               3.2                1.3               0.2\n",
       "3                4.6               3.1                1.5               0.2\n",
       "4                5.0               3.6                1.4               0.2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# i) irisデータセットの読込\n",
    "iris = load_iris()\n",
    "\n",
    "# ii) 特微量をデータフレームに格納して簡易表示\n",
    "iris_features = pd.DataFrame(data = iris.data, columns = iris.feature_names)\n",
    "iris_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.843333</td>\n",
       "      <td>3.057333</td>\n",
       "      <td>3.758000</td>\n",
       "      <td>1.199333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.828066</td>\n",
       "      <td>0.435866</td>\n",
       "      <td>1.765298</td>\n",
       "      <td>0.762238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.300000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.100000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.800000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.350000</td>\n",
       "      <td>1.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.400000</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>1.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.900000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>6.900000</td>\n",
       "      <td>2.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sepal length (cm)  sepal width (cm)  petal length (cm)  \\\n",
       "count         150.000000        150.000000         150.000000   \n",
       "mean            5.843333          3.057333           3.758000   \n",
       "std             0.828066          0.435866           1.765298   \n",
       "min             4.300000          2.000000           1.000000   \n",
       "25%             5.100000          2.800000           1.600000   \n",
       "50%             5.800000          3.000000           4.350000   \n",
       "75%             6.400000          3.300000           5.100000   \n",
       "max             7.900000          4.400000           6.900000   \n",
       "\n",
       "       petal width (cm)  \n",
       "count        150.000000  \n",
       "mean           1.199333  \n",
       "std            0.762238  \n",
       "min            0.100000  \n",
       "25%            0.300000  \n",
       "50%            1.300000  \n",
       "75%            1.800000  \n",
       "max            2.500000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# iii) 特微量の基本統計量を確認\n",
    "iris_features.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sepal length (cm)    0\n",
       "sepal width (cm)     0\n",
       "petal length (cm)    0\n",
       "petal width (cm)     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# iv) 特微量の欠損値数の確認\n",
    "iris_features.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    50\n",
       "1    50\n",
       "0    50\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# v) ラベルを確認 （シリーズに格納、ラベル毎のデータ数表示）\n",
    "iris_label = pd.Series(iris.target)\n",
    "iris_label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(データセットの中身が確認できたので)\n",
    "\n",
    "# vi) 訓練データ と テストデータ に分割\n",
    "from sklearn.model_selection import train_test_split\n",
    "features_train, features_test, label_train, label_test = train_test_split(\n",
    "    iris_features, iris_label, test_size=0.5, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75, 4)\n",
      "(75,)\n",
      "(75, 4)\n",
      "(75,)\n"
     ]
    }
   ],
   "source": [
    "# 学習データとテストデータの構造確認\n",
    "print(features_train.shape)\n",
    "print(label_train.shape)\n",
    "print(features_test.shape)\n",
    "print(label_test.shape)\n",
    "\n",
    "# 特徴量:75行4列 ラベル:75行1列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](https://www.codexa.net/wp-content/uploads/2020/10/スクリーンショット-2020-10-02-17.15.04.png)\n",
    "> START→Yes→Yes→Yes→Yes\n",
    ">\n",
    "> 「Linear SVC」が推奨（サポートベクターマシン）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(max_iter=3000, random_state=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vii) モジュールをインポートし、LinearSVCクラスのインスタンスを作成、fitメソッドで学習\n",
    "from sklearn import svm\n",
    "\n",
    "# LinearSVC インスタンス作成\n",
    "# (データ分割と同様)何回やっても、どの環境で実行しても結果が同じになるように、インスタンス作成時にrandom_state引数を指定\n",
    "Linsvc = svm.LinearSVC(random_state=0, max_iter=3000)\n",
    "\n",
    "# LinearSVC データ学習\n",
    "Linsvc.fit(features_train, label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 0 2 0 2 0 1 1 1 2 1 1 1 1 0 1 1 0 0 2 2 0 0 2 0 0 1 1 0 2 2 0 2 2 1 0\n",
      " 2 1 1 2 0 2 0 0 1 2 2 1 2 1 2 2 1 2 2 2 2 1 2 2 0 2 1 1 1 1 2 0 0 2 1 0 0\n",
      " 1]\n"
     ]
    }
   ],
   "source": [
    "# (モデルにデータを学習させたので)\n",
    "\n",
    "# viii) アヤメ:iris の種類を予測\n",
    "label_pred_linsvc = Linsvc.predict(features_test)\n",
    "print(label_pred_linsvc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(以上)無事に予測完了\n",
    "\n",
    "---\n",
    "\n",
    "チートシート上で　```LinearSVC```がうまく予測できない場合に推奨されている\n",
    "+ ```K近傍法(Kneighbor)```\n",
    "+ ```ロジスティック回帰モデル（LogisticRegression）```（オーソドックスな分類手法）\n",
    "\n",
    "以下それぞれ分類予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 0 2 0 2 0 1 1 1 2 1 1 1 1 0 1 1 0 0 2 1 0 0 2 0 0 1 1 0 2 1 0 2 2 1 0\n",
      " 2 1 1 2 0 2 0 0 1 2 2 1 2 1 2 1 1 2 2 1 2 1 2 1 0 2 1 1 1 1 2 0 0 2 1 0 0\n",
      " 1]\n"
     ]
    }
   ],
   "source": [
    "# K近傍法　モジュールインストール\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# K近傍法　インスタンス作成\n",
    "Kneighbor = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# K近傍法　データ学習\n",
    "Kneighbor.fit(features_train, label_train)\n",
    "\n",
    "# K近傍法　種類予測\n",
    "label_pred_KNeighbor = Kneighbor.predict(features_test)\n",
    "print(label_pred_KNeighbor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 0 2 0 2 0 1 1 1 2 1 1 1 1 0 1 1 0 0 2 1 0 0 1 0 0 1 1 0 2 1 0 2 2 1 0\n",
      " 2 1 1 2 0 2 0 0 1 2 2 1 2 1 2 1 1 2 1 1 2 1 2 1 0 2 1 1 1 1 2 0 0 2 1 0 0\n",
      " 1]\n"
     ]
    }
   ],
   "source": [
    "# ロジスティック回帰\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# ロジスティック回帰　インスタンス作成\n",
    "LogReg = LogisticRegression(random_state=0)\n",
    "\n",
    "# ロジスティック回帰　データ学習\n",
    "LogReg.fit(features_train, label_train)\n",
    "\n",
    "# ロジスティック回帰　種類予測\n",
    "label_pred_LogReg = LogReg.predict(features_test)\n",
    "print(label_pred_LogReg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3つのモデルで予測ができたので、今度はそれぞれの予測モデルの性能を比較。\n",
    "\n",
    "---\n",
    "\n",
    "評価指標として、今回は```正解率(accuracy)```を用い、```混同行列```と併せて表示。 \n",
    "+ 正解率: 全ての予測のうち、正しく分類できていた予測の割合を表示\n",
    "+ 混同行列: 実際のラベルと予測のラベルを行列形式で表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix  = \n",
      " [[21  0  0]\n",
      " [ 0 25  5]\n",
      " [ 0  1 23]]\n",
      "accurary = 0.92\n"
     ]
    }
   ],
   "source": [
    "# LinearSVM予測　混同行列　正解率\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print('confusion matrix  = \\n', confusion_matrix(y_true=label_test, y_pred=label_pred_linsvc))\n",
    "print('accurary =', Linsvc.score(features_test, label_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix  = \n",
      " [[21  0  0]\n",
      " [ 0 29  1]\n",
      " [ 0  2 22]]\n",
      "accurary = 0.96\n"
     ]
    }
   ],
   "source": [
    "# K近傍法予測　混同行列　正解率\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print('confusion matrix  = \\n', confusion_matrix(y_true=label_test, y_pred=label_pred_KNeighbor))\n",
    "print('accurary =', Kneighbor.score(features_test, label_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix  = \n",
      " [[21  0  0]\n",
      " [ 0 29  1]\n",
      " [ 0  4 20]]\n",
      "accurary = 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "# ロジスティック回帰モデル予測　混同行列　正解率\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print('confusion matrix  = \\n', confusion_matrix(y_true=label_test, y_pred=label_pred_LogReg))\n",
    "print('accurary =', LogReg.score(features_test, label_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 回帰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = load_boston()\n",
    "\n",
    "# i) 特徴量データフレームに格納\n",
    "boston_features = pd.DataFrame(data=boston.data, columns=boston.feature_names)\n",
    "\n",
    "boston_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> \n",
    "> + CRIM  : 町の人口あたり犯罪率\n",
    "> + ZN    : 25000平方フィート以上の住宅地の割合\n",
    "> + INDUS : 町ごとの非小売業の土地の割合\n",
    "> + CHAS  : Charles川に関するダミー変数(川に接していたら1、そうでなければ0)\n",
    "> + NOX   : 一酸化窒素濃度（1000万分の1）\n",
    "> + RM    : 住居当たりの平均部屋数\n",
    "> + AGE   : 1940年より前に建設された物件の割合\n",
    "> + DIS   : ボストンの5つの雇用センターまでの重み付けされた距離\n",
    "> + RAD   : 放射状高速道路へのアクセスしやすさ\n",
    "> + TAX   : 10000ドル当たりの固定資産税総額\n",
    "> + PTRATIO: 町ごとの生徒と教師の比率\n",
    "> + B     : 1000(Bk – 0.63)^2 ※Bkは町の黒人比率\n",
    "> + LSTAT : 低所得の人々の割合\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_features.describe()    # 特徴量の基本統計量を確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_features.isnull().sum()    # 特徴量の欠損値確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ii) ターゲットをpandasのSeriesというデータ構造に格納\n",
    "boston_target = pd.Series(data=boston.target)\n",
    "\n",
    "boston_target.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iii) 学習データ(訓練データ)　テストデータ　に分割\n",
    "from sklearn.model_selection import train_test_split\n",
    "features_train, features_test, target_train, target_test = train_test_split(boston_features, boston_target, test_size=0.5, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ![img](https://www.codexa.net/wp-content/uploads/2020/10/スクリーンショット-2020-10-02-17.16.29.png)\n",
    "> START→Yes→No→Yes→Yes\n",
    "\n",
    "> few features should be important(少数の特徴量が重要である)\n",
    "\n",
    "> この質問に関しては、データを確認しただけでは判断がつかない...Yes No それぞれ実装\n",
    "\n",
    "> + ```Lasso```\n",
    "> + ```RidgeRegression```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso\n",
    "from sklearn.linear_model import Lasso\n",
    "Lasso = Lasso(alpha=0.1, random_state=0)    # インスタンス作成\n",
    "Lasso.fit(features_train, target_train)    # 学習\n",
    "target_pred_Lasso = Lasso.predict(features_test)    # 予測\n",
    "print(target_pred_Lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RidgeRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "Ridge = Ridge(alpha=0.5, random_state=0)    # インスタンス作成\n",
    "Ridge.fit(features_train, target_train)    # 学習\n",
    "target_pred_Ridge = Ridge.predict(features_test)    # 予測\n",
    "print(target_pred_Ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cf.線形回帰\n",
    "from sklearn.linear_model import LinearRegression\n",
    "LinReg = LinearRegression()\n",
    "LinReg.fit(features_train, target_train)\n",
    "target_pred_LinReg = LinReg.predict(features_test)\n",
    "print(target_pred_LinReg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# （３モデルの予測が出揃ったので）\n",
    "\n",
    "# 評価指標\n",
    "# （決定係数：「実際の値のうち予測モデルで説明できた割合」を表す評価指標で０〜１の範囲で変動）\n",
    "\n",
    "# ① Lassoモデルによる回帰の評価（決定係数の表示）\n",
    "print('R-squared : ', Lasso.score(features_test, target_test))\n",
    "# ② Ridgeモデルによる回帰の評価（決定係数の表示）\n",
    "print('R-squared : ', Ridge.score(features_test, target_test))\n",
    "# Cf. ③ 線形回帰モデルによる回帰の評価（決定係数の表示）\n",
    "print('R-squared : ', LinReg.score(features_test, target_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデル間で性能に大差はないという結果に。\n",
    "\n",
    "線形モデルが最も性能が良かったですが、チートシートにしたがって用いた二つのモデルもまずまずの性能を出せたと言える.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの評価と選択\n",
    "\n",
    "+ 混同行列\n",
    "+ クロスバリデーション（交差検証):\n",
    ">データセットを訓練データとテストデータに分割してモデルに学習させてスコアを出すというプロセスを複数回繰り返し、全てのスコアの平均で性能を測る"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit, cross_val_score\n",
    "\n",
    "# データセットをランダムに５分割するための変数定義\n",
    "cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n",
    "\n",
    "# ① Lasso\n",
    "# cv を用いてクロスバリデーション実行\n",
    "score = cross_val_score(Lasso, boston_features, boston_target, cv=cv)\n",
    "print(score)\n",
    "print('R-squared_Average : {0:.2f}'.format(score.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ② Ridge\n",
    "# cv を用いてクロスバリデーション実行\n",
    "score = cross_val_score(Ridge, boston_features, boston_target, cv=cv)\n",
    "print(score)\n",
    "print('R-squared_Average : {0:.2f}'.format(score.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ③ LinReg\n",
    "# cv を用いてクロスバリデーション実行\n",
    "score = cross_val_score(LinReg, boston_features, boston_target, cv=cv)\n",
    "print(score)\n",
    "print('R-squared_Average : {0:.2f}'.format(score.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上の結果から、今回の分類に関しては、Ridgeモデル、線形モデルの方がLassoモデルよりも優れていることを確認できました。\n",
    "\n",
    "---\n",
    "\n",
    "モデルの評価と選択のための手法は他にもあります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [ハイパーパラメータチューニング](https://www.codexa.net/hyperparameter-tuning-python/)(3手法)\n",
    "> 少し乱暴な言い方をすると機械学習のアルゴリズムの「設定」\n",
    "---\n",
    "\n",
    "### 1. Grid Serach(グリッドサーチ)\n",
    "\n",
    "> 与えられたハイパーパラメータの候補の値の全パターンのモデル構築を行う手法\n",
    "\n",
    "> e.g. 設定Aと設定Bのハイパーパラメータを調整する場合、設定Aには「1、3、5」の値、設定Bには「True、False」の値を候補として指定(3×２＝全6回のモデル訓練が異なる設定の値で実行される)。\n",
    "\n",
    ">【メリット】\n",
    "調整する値の「あたり」が付いている場合は◎\n",
    "調整する値の数が少ない場合は◎\n",
    "\n",
    ">【デメリット】\n",
    "モデル訓練回数が増えるので時間が掛かる\n",
    "計算コストが非常に高い\n",
    "\n",
    "### 2. Random Search(ランダムサーチ)\n",
    "\n",
    "> 候補の値をランダムに組み合わせたモデル訓練を行いハイパーパラメータを検証する手法\n",
    "\n",
    "> 例えば候補Aには10個の値、候補Bには5個、候補Cは30個の値があるとします。\n",
    "グリッドサーチでは全組み合わせ1500回（10 x 5 x 30）のモデル訓練を行います。仮に1回の訓練で30分要するとした場合31日と現実的でないことに。。。そこでランダムサーチが役に立ちます。ランダムサーチで異なる組み合わせのハイパーパラメータを用いてモデル訓練を行い検証。\n",
    "\n",
    "> 【メリット】\n",
    "調整する値が多くても対応することが可能\n",
    "\n",
    "> 【デメリット】\n",
    "ランダムに検証するので「運任せ」の要素あり\n",
    "\n",
    "\n",
    "### 3. Bayesian Optimization（ベイズ最適化）\n",
    "\n",
    "> ベイズ最適化をざっくりと解説すると「前回の結果を基に次に調べる値を決めていく」手法。\n",
    "（不確かさを利用して次に探索を行うべき値を探していく最適化アルゴリズムの一種。目的関数（Acquisition Function）を推定する代理モデル（Surrogate Model）にはガウス過程が用いられる）\n",
    "\n",
    "> 2つの戦略を使って最適化を順次的実行\n",
    "> + 「Exploration（探索）」\n",
    "> + 「Exploitation（活用）」\n",
    "\n",
    "> e.g. よく行く居酒屋で飲み物を選ぶ時を想像してみてください。先週来た時は泡盛を頼み、とても美味しいのを覚えています。今回も似たような泡盛を頼めば恐らく前回と同様に楽しめることが想像できます。これがExploitation（活用）です。しかしメニューには日本酒や焼酎など、今まで頼んだことが無い飲み物もたくさんあります。もしかすると泡盛よりも自分好みのお酒があるかもしれません。そこで今回は日本酒を頼んでみることにします。これがExploration（探索）\n",
    "\n",
    "\n",
    "cf.[ハイパーパラメータとは？チューニングの手法を徹底解説（XGBoost編）](https://www.codexa.net/hyperparameter-tuning-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i)　インポート\n",
    "import pandas as pd\n",
    "\n",
    "#Scikit-learn\n",
    "from sklearn.model_selection import train_test_split #訓練、テストデータ分割\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix #正解率　混同行列\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, KFold\n",
    "\n",
    "# XGBoost\n",
    "import xgboost as xgb\n",
    "\n",
    "# Marplotlibのインライン表示\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!brew install libomp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i)　インポート\n",
    "import pandas as pd\n",
    "\n",
    "#Scikit-learn\n",
    "from sklearn.model_selection import train_test_split #訓練、テストデータ分割\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix #正解率　混同行列\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, KFold\n",
    "\n",
    "# XGBoost\n",
    "import xgboost as xgb\n",
    "\n",
    "# Marplotlibのインライン表示\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ii) データフレームセット\n",
    "df = pd.read_csv('../csv/HRDataset_v14.csv', index_col=['Employee_Name'])\n",
    "\n",
    "#　必要なデータを列挙\n",
    "df = df[['Termd', 'Position', 'Sex', 'MaritalDesc', 'RaceDesc', \n",
    "        'Department', 'ManagerName', 'RecruitmentSource', \n",
    "        'EngagementSurvey','EmpSatisfaction', 'SpecialProjectsCount', ]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA（探索的データ解析）\n",
    "\n",
    "---\n",
    "\n",
    "余力があれば、以下のように更に深いEDAも！\n",
    "+ 男女別（カラムSex）間で辞職率に違いはありますか？（ヒント：GroupBy）\n",
    "+ 従業員の満足度（EmpSatisfaction）と辞職（Termd）に相関性はありますか？（ヒント：corrメソッド）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 簡単なEDA（探索的データ解析）を行いデータを確認\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "（余談） 綺麗なデートセットであったが、もしも欠損値が存在する場合。\n",
    "\n",
    "### 「前処理」\n",
    "\n",
    "```Python\n",
    "df[df.duplicated(keep=False)].head()\n",
    "```\n",
    "\n",
    "> pandasのデータフレームの```.duplicated()```メソッドは重複した行があればTrue、それ以外はFalseを戻す。\n",
    "> duplicatedメソッドのkeep引数は重複した行の抽出方法を指定することが可能。(Falseと指定した場合、重複した全ての行をTrueとして戻す。よって重複行を表示させる)\n",
    "\n",
    "```python\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "df.duplicated().sum()\n",
    "[out] 0 #欠損データが０個に\n",
    "```\n",
    "\n",
    "> ```.dropna()```メソッドは欠損値を含む行を除外したデータフレームを戻す。\n",
    "> inplace引数（初期値 False）をTrue(データフレームに直接変更を加える可)\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Position'].value_counts()[0:10].plot(kind='pie', figsize=(4, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['RecruitmentSource'].value_counts()[0:10].plot(kind='pie', figsize=(4, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Termd'].value_counts(dropna=False) # 「０:！辞職」「１：辞職」\n",
    "# dropna引数（初期値 True）は欠損値の扱いを制御する引数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前処理\n",
    "---\n",
    "\n",
    "機械学習で使われる多くのモデリング手法は文字列の値を訓練データとして使用することは出来ません。\n",
    "\n",
    "+ ダミー変数\n",
    " >ダミー変数とは数字ではないデータを数字に変換する手法をさす。\n",
    " > 文字列の値を持つ値をダミー変数に変換\n",
    " > + Position（肩書き）\n",
    " > + Sex（性別）\n",
    " > + MaritalDesc（結婚歴）\n",
    " > + RaceDesc（人種）\n",
    " > + Department（所属部署）\n",
    " > + ManagerName（上司の名前）\n",
    " > + RecruitmentSource（採用経由）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iii) 文字列データを編集（ダミー変数）\n",
    "dummy_cols = ['Position', 'Sex', 'MaritalDesc', 'RaceDesc', \n",
    "              'Department', 'ManagerName', 'RecruitmentSource']\n",
    "df = pd.get_dummies(df, columns=dummy_cols)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iv) x:説明変数（特徴量）　y：目的変数　へ分割\n",
    "x = df.drop(['Termd'], axis=1) #Termd以外は特徴量\n",
    "y = df['Termd'] # 従業員の辞職を示すカラムTermd\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v） train:訓練データ　test:テストデータ　へ分割\n",
    "seed = 42\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=seed)\n",
    "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vi) 目的変数（カラムTermdの値）の分布を確認\n",
    "pd.concat([y_train.value_counts(), y_test.value_counts()], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    訓練データ（y_train）には、辞職76名、現職141名。\n",
    "    テストデータ（y_test）には辞職2８名、現職66名なのが確認できます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⓪ベースライン\n",
    "---\n",
    "\n",
    "+ XGBoost（XGBoostは勾配ブースティングのフレームワーク）を使い人事データセットの説明変数を用いて退職の有無（カラムTermd）を分類推測を行う。\n",
    ">勾配ブースティングの学習プロセスは繰り返し処理（イテレーション）\n",
    ">一つ前の学習結果の誤差を繰り返し学習する手法\n",
    "\n",
    "+ XGBoostのハイパーパラメータの数は細かいのを含めると50以上。\n",
    "> 中でも特にモデリングに影響の高い以下のハイパーパラメータの調整を行う。\n",
    "> + min_child_weight\n",
    "> + max_depth\n",
    "> + colsample_bytree\n",
    "> + subsample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vii) XGBoostへ設定するハイパーパラメータを辞書型で作成\n",
    "params = {'metric':'error',\n",
    "          'objective':'binary:logistic',\n",
    "          'n_estimators':50000,     #何回繰り返すか?\n",
    "          'booster': 'gbtree',\n",
    "          'learning_rate':0.01,\n",
    "          'min_child_weight':1,\n",
    "          'max_depth':5,\n",
    "          'random_state':seed,\n",
    "          'colsample_bytree':1,\n",
    "          'subsample':1,\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# viii) ベースライン（モデル）の訓練\n",
    "cls = xgb.XGBClassifier()\n",
    "cls.set_params(**params)\n",
    "cls.fit(x_train,\n",
    "        y_train,\n",
    "        early_stopping_rounds=50, # 50回以上モデル改善ない場合に学習プロセス停止　#学習完了毎時テストデータモデルの評価を実行、評価指標が一定の回数を改善しなくなった時点で学習ストップ\n",
    "        eval_set=[(x_test, y_test)],\n",
    "        eval_metric='error',\n",
    "        verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('best score : ', cls.best_score)    # 最もスコアが良かった回のスコア\n",
    "print('best iter : ', cls.best_iteration)    # 最もスコアが良かった回数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ix) 予測 正解率 混同行列\n",
    "pred_1 = cls.predict(x_test)\n",
    "\n",
    "baseline = accuracy_score(y_test, pred_1)\n",
    "print(baseline)\n",
    "\n",
    "confusion_matrix(y_test, pred_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回使用したハイパーパラメータの値では正解率は ```7６.59%``` となっているのが確認できる。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ①グリッドサーチ（GridSearch）\n",
    "> ハイパーパラメータをチューニング手法（GridSearchCV）でチューニング\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x） グリッドサーチ ： 16通り(2×2×2×2)の異なるハイパーパラメータの値を持つXGBoostの分類器の訓練\n",
    "cv_params = {'metric':['error'],    # [検証非対称]検証する対象ではないので1つの値のみを設定\n",
    "             'objective':['binary:logistic'],    # [検証非対称]\n",
    "             'n_estimators':[50000],\n",
    "             'random_state':[seed],\n",
    "             'booster': ['gbtree'],\n",
    "             'learning_rate':[0.01],\n",
    "             'min_child_weight':[1,5],    # [検証対称]検証するハイパーパラメータには複数の候補値を設定　２つ\n",
    "             'max_depth':[1,3],    # [検証対称]２つ\n",
    "             'colsample_bytree':[0.5,1.0],    # [検証対称]２つ\n",
    "             'subsample':[0.5,1.0]    # [検証対称]２つ\n",
    "            }\n",
    "\n",
    "cls = xgb.XGBClassifier()\n",
    "# Scikit-learnのGridSearchCV関数(CV:Cross Validation/交差検証)\n",
    "cls_grid = GridSearchCV(cls, cv_params, cv=KFold(2, random_state=seed), scoring='accuracy', iid=False)\n",
    "cls_grid.fit(x_train,\n",
    "             y_train,\n",
    "             early_stopping_rounds=50,\n",
    "             eval_set=[(x_test, y_test)],\n",
    "             eval_metric='error',\n",
    "             verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cls_grid.best_params_) # 交差検証で得た最も評価スコアが良いハイパーパラメータの値を戻す\n",
    "print('best score : ', cls_grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xi) 予測 正解率 混同行列\n",
    "pred_2 = cls_grid.best_estimator_.predict(x_test)\n",
    "\n",
    "grid_score = accuracy_score(y_test, pred_2)\n",
    "print(grid_score)\n",
    "\n",
    "confusion_matrix(y_test, pred_2) # モデルがどのような推測結果を戻したのか混同行列で確認"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回使用したハイパーパラメータの値では正解率は ```６9.14%``` となっているのが確認できる。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ②ランダムサーチ（RandomizedSearchCV）\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xii) ランダムサーチ\n",
    "# グリッドサーチでは各パラメータで2つずつの候補、今回はそれぞれ10個の候補の値を指定\n",
    "cv_params = {'metric':['error'],\n",
    "             'objective':['binary:logistic'],\n",
    "             'n_estimators':[50000],\n",
    "             'random_state':[seed],\n",
    "             'boosting_type': ['gbdt'],\n",
    "             'learning_rate':[0.01],\n",
    "             'min_child_weight':[1,2,3,4,5,6,7,8,9,10],\n",
    "             'max_depth':[1,2,3,4,5,6,7,8,9,10],\n",
    "             'colsample_bytree':[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0],\n",
    "             'subsample':[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0],\n",
    "            }\n",
    " \n",
    "cls = xgb.XGBClassifier()\n",
    "cls_rdn = RandomizedSearchCV(cls,\n",
    "                             cv_params,\n",
    "                             cv=KFold(2, random_state=seed),\n",
    "                             random_state=seed,\n",
    "                             n_iter=30,\n",
    "                             iid=False,\n",
    "                             scoring='accuracy')\n",
    "cls_rdn.fit(x_train,\n",
    "            y_train,\n",
    "            early_stopping_rounds=50,\n",
    "            eval_set=[(x_test, y_test)],\n",
    "            eval_metric='error',\n",
    "            verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cls_rdn.best_params_)\n",
    "print('best score : ', cls_rdn.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xiii) 予測 正解率 混同行列\n",
    "pred_3 = cls_rdn.best_estimator_.predict(x_test)\n",
    "\n",
    "rdn_score = accuracy_score(y_test, pred_3)\n",
    "print(rdn_score)\n",
    "\n",
    "confusion_matrix(y_test, pred_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回使用したハイパーパラメータの値では正解率は ```70.21%``` となっているのが確認できる。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ③ベイズ最適化（BayesianOptimization）\n",
    "> ベイズ最適化ではオープンソースのライブラリが複数ある。\n",
    "> 今回は、「BayesianOptimization」を利用してXGBoostのハイパーパラメータチューニングを行う。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bayesian-optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xiv) ベイズ最適化\n",
    "# 関数定義：調整を行うハイパーパラメータを引数、モデル訓練・テストデータを使った推測値を算出、正解率の値を戻す関数定義\n",
    "def xgb_evaluate(min_child_weight, subsample, colsample_bytree, max_depth):\n",
    "    params = {'metric': 'error',\n",
    "              'objective':'binary:logistic',\n",
    "              'n_estimators':50000,\n",
    "              'random_state':42,\n",
    "              'boosting_type':'gbdt',\n",
    "              'learning_rate':0.01,              \n",
    "              'min_child_weight': int(min_child_weight),\n",
    "              'max_depth': int(max_depth),\n",
    "              'colsample_bytree': colsample_bytree,\n",
    "              'subsample': subsample,\n",
    "             }\n",
    "    # 学習\n",
    "    cls = xgb.XGBClassifier()\n",
    "    cls.set_params(**params)\n",
    "    cls.fit(x_train,\n",
    "            y_train,\n",
    "            early_stopping_rounds=50,\n",
    "            eval_set=[(x_test, y_test)],\n",
    "            eval_metric='error',\n",
    "            verbose=0)\n",
    "    # 予測\n",
    "    pred = cls.predict(x_test)\n",
    "    score = accuracy_score(y_test, pred)\n",
    "    return score\n",
    "\n",
    "\n",
    "# ベイズ最適化\n",
    "xgb_bo = BayesianOptimization(xgb_evaluate, \n",
    "                              {'min_child_weight': (1,20), #  1~20に属する値を候補の値として探索\n",
    "                               'subsample': (.1,1),\n",
    "                               'colsample_bytree': (.1,1),\n",
    "                               'max_depth': (1,50)},\n",
    "                              random_state=10)\n",
    "\n",
    "# 検証実行\n",
    "xgb_bo.maximize(init_points=15, n_iter=50, acq='ei')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_params = xgb_bo.max['params']    # max属性:最評価スコア結果取得\n",
    "optimized_params['max_depth'] = int(optimized_params['max_depth'])\n",
    "optimized_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 固定されたハイパーパラメータを変数へ格納\n",
    "fixed_params = {'metric':'error',\n",
    "                'objective':'binary:logistic',\n",
    "                'n_estimators':50000,\n",
    "                'random_state':seed,\n",
    "                'booster': 'gbtree',\n",
    "                'learning_rate':0.01}\n",
    "\n",
    "# 訓練\n",
    "cls = xgb.XGBClassifier()\n",
    "cls.set_params(**fixed_params, **optimized_params)\n",
    "cls.fit(x_train,\n",
    "        y_train,\n",
    "        early_stopping_rounds=50,\n",
    "        eval_set=[(x_test, y_test)],\n",
    "        eval_metric='error',\n",
    "        verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x) 予測 正解率 混同行列\n",
    "pred_4 = cls.predict(x_test)\n",
    "\n",
    "baseline = accuracy_score(y_test, pred_4)\n",
    "print(baseline)\n",
    "\n",
    "confusion_matrix(y_test, pred_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回使用したハイパーパラメータの値では正解率は ```70.21%``` となっているのが確認できる。\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

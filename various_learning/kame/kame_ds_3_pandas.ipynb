{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "Pandas"
    ]
   },
   "source": [
    "# Pandas\n",
    "```index```\n",
    "+ [データサイエンスのためのPython入門⑩〜PandasインストールからSeriesの使い方〜](https://datawokagaku.com/pandas_series/)\n",
    "+ [データサイエンスのためのPython入門11〜PandasのDataFrameを作る．CSVファイルを読み込む〜](https://datawokagaku.com/dataframe/)\n",
    "+ [データサイエンスのためのPython入門12〜DataFrameの基本的な使い方(head, describe, Seriesの取得など)〜](https://datawokagaku.com/dataframe_howto_1/)\n",
    "+ [データサイエンスのためのPython入門13〜DataFrameのフィルタ操作の基本(超重要)〜](https://datawokagaku.com/dataframe_filter/)\n",
    "+ [データサイエンスのためのPython入門14〜DataFrameの欠損値NaNに対応する〜](https://datawokagaku.com/dataframe_nan/)\n",
    "+ [データサイエンスのためのPython入門15〜DataFrameのgroupbyをマスターする〜](https://datawokagaku.com/dataframe_groupby/)\n",
    "+ [データサイエンスのためのPython入門16〜DataFrameのテーブル結合を完全解説(merge, join, concat)〜](https://datawokagaku.com/dataframe_merge/)\n",
    "+ [データサイエンスのためのPython入門17〜DataFrameの重要関数(apply, unique, value_counts)を超わかりやすく解説〜](https://datawokagaku.com/dataframe_apply/)\n",
    "+ [データサイエンスのためのPython入門18〜DataFrameのその他頻出関数(to_csv, iterrows, sort_values)を解説〜](https://datawokagaku.com/dataframe_func1/)\n",
    "+ [データサイエンスのためのPython入門19〜DataFrameのその他頻出関数(pivot_table, xs)を解説〜](https://datawokagaku.com/dataframe_func2/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10: インストールからSeriesの使い方\n",
    "    Pandasはデータ操作や解析を目的として作られたPythonライブラリで、NumPyをなかで使っている。\n",
    "    とりわけ表形式のデータ処理が得意で、エクセルで処理するようなことをPythonでできる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### | Pandasをimport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandasの所在確認\n",
    "pd.__file__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### | Seriesを使う\n",
    "    PandasにあるSeriesというクラスを使う。\n",
    "    Series：表形式のデータの各行、カラムを切り取ったデータを表すデータ形式\n",
    "\n",
    "<img src='https://datawokagaku.com/wp-content/uploads/2020/01/dataframe_yougo.png' width=65%>\n",
    "\n",
    "+ Table(テーブル):表形式のデータ\n",
    "+ header(ヘッダー):表の一番うえに並んでいるの\n",
    "+ column(カラム):ヘッダーを構成する一つ一つ\n",
    "+ row(ロウ):表には色々なデータがずらっとヘッダーのカラム順に合わせて並んでいる一行一行\n",
    "+ recode(レコード):行のこと\n",
    "\n",
    "<img src='https://datawokagaku.com/wp-content/uploads/2020/01/dataframe_series.png' width=65%>\n",
    "\n",
    "+ DataFrame(データフレーム):Pandasでは、この表をDataFrameというデータ構造で扱う。\n",
    "+ Series(シリーズ):各行をSeriesというデータ構造を使って扱う。（縦に切り取ってもSeriesになる）\n",
    "（つまり，Seriesというデータ構造が集まってDataFrameというデータ構造になるイメージ）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 作り方は簡単で、dictionaryを作ってそれをpd.Series()に入れるだけ!\n",
    "data = {\n",
    "    'name':'John',\n",
    "    'sex':'mael',\n",
    "    'age':22\n",
    "}\n",
    "\n",
    "john_s = pd.Series(data)\n",
    "print(john_s)\n",
    "print('\\n Johns age is {}.'.format(john_s['age']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy Arraysを使って作ることも可能\n",
    "import numpy as np\n",
    "array = np.array([100, 200, 300])\n",
    "array = pd.Series(array)\n",
    "print(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = array.rename(index={0:'a',1:'b',2:'c'})\n",
    "print(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11: DataFrameを作る CSVファイルを読み込む"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### | DataFrameの作り方"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - ndarray → df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "\n",
    "ndarray = np.random.randint(5, size=(5, 4))\n",
    "pd.DataFrame(data=ndarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['a', 'b', 'c', 'd']\n",
    "index = np.arange(0, 50, 10)\n",
    "pd.DataFrame(data=ndarray, index=index, columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - dictionary → df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = {\n",
    "    'name':'John',\n",
    "    'sex':'male',\n",
    "    'age':22\n",
    "}\n",
    "data2 = {\n",
    "    'name':'Zack',\n",
    "    'sex':'male',\n",
    "    'age':30\n",
    "}\n",
    "data3 = {\n",
    "    'name':'Emily',\n",
    "    'sex':'female',\n",
    "    'age':32\n",
    "}\n",
    "pd.DataFrame([data1, data2, data3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - 各.file → df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('url')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12: DataFrameの基本的操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - .head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#統計量確認\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - .columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### | [](ブラケット)特定カラム抽出（Series）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_df = df['Age']\n",
    "age_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(age_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Age', 'Parch', 'Fare']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -.iloc[int]\n",
    "(index location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">```memo```\n",
    ">\n",
    ">     基本はカラムは文字列，indexは数値になると思います.\n",
    ">     たまにindexにID（タイタニックでいうと PassengerId ）を指定することもありますが,\n",
    ">    行の取得にはほとんどの場合 ```.iloc[] ```を使います． 私は滅多に ```loc[] ```は使わないです．\n",
    ">\n",
    ">     カラムはできるだけ意味のある文字列にしてください． 意味を持たない数値はややこしいのでやめましょう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[888]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[888]['Age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NaNというのはnp.nanでありNoneではないことに注意!!!\n",
    "import numpy as np\n",
    "np.isnan(df.iloc[888]['Age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[888]['Age'] is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('train.csv').iloc[5:10] # Slicing\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - .drop()\n",
    ">     複数のカラムを落としたい場合はカラムをリストにして渡してください．\n",
    ">     また， dropしても元のdfは変更されません．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.drop(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.drop('Age', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.drop(['Age', 'Parch'], axis=1) #元のdfを上書きする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "# df上書きパターン１ : inplace=True\n",
    "df = pd.read_csv('train.csv')\n",
    "df.drop(['Age', 'Cabin'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# df上書きパターン２ : 同名変数に再代入\n",
    "df = pd.read_csv('train.csv')\n",
    "df = df.drop(['Age', 'Cabin'], axis=1)\n",
    "```\n",
    "\n",
    "\n",
    ">```memo```\n",
    ">\n",
    ">      私は後者を使います． 理由は， ぱっと見でわかるしデータサイエンスではよく使う書き方だからです．\n",
    ">      他のプログラミング言語を勉強した人からすると， 後者は違和感のある書き方だと思います．\n",
    ">      なるべく結果は違う変数にして変数名を変えて意味を持たせることが多いと思います．\n",
    ">      例えば df_drop という変数名にしたりとか\n",
    ">      しかし， データサイエンスでは一つの変数が巨大なメモリを使っているケースが多いです．\n",
    ">      タイタニックのデータは練習用なので小さいですが，実業務では普通に1万レコードとかになったりします．\n",
    ">      大きなデータを複数のメモリでコピーしていくとすぐにメモリリーク(メモリ不足)を起こしてしまうので， 同じメモリを使いまわすのが定石です．\n",
    ">      (ここではdfというオブジェクトを使い回しています．)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13: DataFrameのフィルタ操作の基本(超重要)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### | (超重要）特定条件フィルタ(filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 条件付きでSeriesを取得\n",
    "df['Survived']==1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[filterの条件]で， ある条件に該当したレコードだけが返ってきます．(SQLでいうwhere句のような)\n",
    "filter = df['Survived']==1\n",
    "df[filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[filter].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 60才以上のレコードに絞る\n",
    "fill_age60 = df['Age']>=60\n",
    "# 1stクラスのレコードに絞る\n",
    "fill_pclass = df['Pclass']==1\n",
    "# 女性のレコードに絞る\n",
    "fill_sex = df['Sex'] == 'female'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - ()&() , ()|()　\n",
    "> 複数条件フィルタ\n",
    "\n",
    ">     Pythonだとandやorを使って条件を作りますが，\n",
    ">     DataFrameのフィルタ操作では & と | であることに注意!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 60歳以上の女性のデータフレーム\n",
    "df[(df['Age']>=60) & (df['Sex']=='female')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1stクラスの人、もしくは10歳未満の子どものデータフレーム\n",
    "df[(df['Pclass']==1) | (df['Age']<10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - ~ （スクィグル） \n",
    "> NOT演算フィルタ\n",
    "\n",
    ">     条件の真偽が逆転します(NOT演算, inversionという．)\n",
    ">     これは特に「値がbooleanのカラムでフィルタする時」によく使う。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e.g. Survivedが真偽値の場合\n",
    "data = [\n",
    "    {'Name':'John', 'Survived':True},\n",
    "    {'Name':'Emily', 'Survived':False},\n",
    "    {'Name':'Ben', 'Survived':True},\n",
    "]\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Survived']==True] #値は既にBoolのため、==Trueの場合は省略可能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = df[df['Survived']==False]\n",
    "s = df[~df['Survived']]\n",
    "print(f, '\\n', s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### | index変更"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - .reset_index()\n",
    "> 再度indexを割り振る"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('train.csv')\n",
    "df2 = df2[df2['Sex']=='male']\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.reset_index()\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - .set_index()\n",
    "> 特定のカラムをindexにする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nameをindexにしたい\n",
    "df2 = df2.set_index('Name')\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14: DataFrameの欠損値NaNに対応"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### | DataFrameのNaNについて\n",
    "\n",
    ">```復習```\n",
    ">\n",
    "> + NaNはNot A Numberの略\n",
    "> + np.nanと同じ\n",
    "> + NaN判定には np.isnan() を使う(後述: pd.isna()もあります．)\n",
    "> + Noneとは別物\n",
    "> + DataFrameでは基本NaNが使われる．\n",
    "\n",
    "> ちなみに， csvやエクセルで値が空白だと読み込んだ時にNaNになります．Noneではないことに注意!!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df = pd.read_csv('train.csv')\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### | DataFrameのNaN用の関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - .dropna()\n",
    "> デフォルト axis=0 : NaNのあるレコード(行)を落とす\n",
    ">> axis=1 を引数にいれるとNaNを含むカラム(列)をdrop\n",
    "\n",
    "```memo```\n",
    "\n",
    "    モデルを組む際に， データ数を減らさずにデータを説明する変数(説明変数)を減らす作戦のときに使いますが，\n",
    "    「NaNが一つでもあるのでその説明変数を減らす」ということはまずありません．\n",
    "    どの説明変数がモデル構築に重要なのかというのは非常に重要かつ慎重に考えるべき問題です．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna().tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特定カラムのNaNのレコードだけ！ (実行業務頻出)\n",
    "# リスト形式で渡すので注意． たとえ一つのカラムでも， リストで囲って渡す\n",
    "df.dropna(subset=['Age'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - .fillna(value)\n",
    "> NaNに特定のvalueを代入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna('This is it!').tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Age'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AgeのカラムのNaNだけ、Ageの平均値を代入 \n",
    "df['Age'].fillna(df['Age'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Age'] = df['Age'].fillna(df['Age'].mean())\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - pd.isna()\n",
    "\n",
    "> ```復習```\n",
    ">\n",
    ">     np.isnan() で， DataFrameの全ての値のNaNの判定が可能です．\n",
    ">     しかし, np.isnan() だといちいちループで回さないといけないですし， stringsを入れるとエラーになったり， 使い勝手が悪いです．\n",
    ">\n",
    ">     そこで, DataFrameの中の値のNaN判定には pd.isna() を使うといいです．\n",
    ">     (pd.isnull() も同じです． 最近名前が変わって pd.isna() が実装されました．特に理由がなければ pd.isna() を使いましょう．np.isnanと違って最後のnがないので注意です．)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('train.csv')\n",
    "pd.isna(df2).tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seriesに対してよく使いますね．NaN判定の結果を別カラムで持ちたい時とか↓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#　Cabin_nanカラムを使いして，CabinのNaN判定結果を代入する\n",
    "df2['Cabin_nan'] = pd.isna(df2['Cabin'])\n",
    "df2.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15: DataFrameのgroupby"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### | groupby"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - .groupby().関数()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pclassでgroupby\n",
    "df.groupby('Pclass').mean() # .sum() .count() .descrive()等々使える"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"Age\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 試しに、階級１をフィルタリングして統計量を算出\n",
    "df[df['Pclass'] == 1].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 更に、meanにフォーカスしたいので、\n",
    "df[df['Pclass'] == 1].describe().loc['mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 当然これもDataFrameなので．以下のようにカラムを指定して取り出すことができます．\n",
    "df.groupby('Pclass').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('Pclass').describe()['Age']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - set_option()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "# カラムを省略せずに表示\n",
    "pd.set_option('display.max_columns', None)\n",
    "# 行を省略せずに表示\n",
    "pd.set_option('display.max_rows', None)\n",
    "```\n",
    "\n",
    "戻し方\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### | (上級者向け)groupbyの結果をfor文でまわす\n",
    "\n",
    "    .groupby() の結果はfor文で回すことができ，(index, groupbyされたDataFrame)のタプルの形で回せます．\n",
    "    \n",
    "    どういうことかというと以下の例をみてください． \n",
    "    i と group_df にはそれぞれ1, 2, 3および Pclass==1, ==2, ==3でフィルタされたときのDataFrameが格納されています．\n",
    "    （例として len() で各DataFrameのレコード数を表示してます．）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, group_df in df.groupby('Pclass'):\n",
    "    print(\"{}: group_df's type is {} and has {}\".format(i, type(group_df), len(group_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各Pclassのグループの中で，各レコードが何番目にFareが高いか数字を振ってみ\n",
    "df = pd.read_csv('train.csv')\n",
    "results = []\n",
    "for i, group_df in df.groupby('Pclass'):\n",
    "    sorted_group_df = group_df.sort_values('Fare')\n",
    "    sorted_group_df['RankInPClass'] = np.arange(len(sorted_group_df))\n",
    "    results.append(sorted_group_df)\n",
    "result_df = pd.concat(results)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16: DataFrameのテーブル結合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### | 表の結合とは？\n",
    "\n",
    "> + 特定のカラムやindexをKeyにして結合する\n",
    "> + DataFrameを単純に横に(もしくは縦に)結合する（ガッチャンコさせる）\n",
    "\n",
    "    mergeとconcatでは圧倒的にmergeの方が出てきます．\n",
    "    mergeもconcatも元のDataFrameを更新しないので ，新たな変数か元の変数に再代入する必要あり\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df1 = pd.DataFrame({ 'Key': ['k0', 'k1', 'k2'],\n",
    "        'A': ['a0', 'a1', 'a2'],\n",
    "        'B': ['b0', 'b1', 'b2']})\n",
    "\n",
    "df2 = pd.DataFrame({ 'Key': ['k0', 'k1', 'k2'],\n",
    "        'C': ['c0', 'c1', 'c2'],\n",
    "        'D': ['d0', 'd1', 'd2']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - df.merge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特定のカラムやindexをKeyにして結合する\n",
    "df1.merge(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - pd.concat()\n",
    "> concatenateの略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrameを単純に横に(もしくは縦に)結合する（ガッチャンコさせる）\n",
    "# 縦\n",
    "pd.concat([df1, df2], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 横\n",
    "pd.concat([df1, df2], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### | .merge()の使い方をマスタ-\n",
    "\n",
    "重要な引数：\n",
    "\n",
    "<img src='https://datawokagaku.com/wp-content/uploads/2020/02/params_merge-300x241.png' width=35%>\n",
    "\n",
    "> + how : どう結合するか→{‘left’, ‘right’, ‘outer’, ‘inner’}, デフォルトは ‘inner’\n",
    "> + on : keyにするカラムを指定（どちらのDataFrameにも存在するカラム）．指定をしないと共通のカラムで結合される\n",
    "> + left_on：leftのDataFrameのkeyにするカラム\n",
    "> + right_on：rightのDataFrameのkeyにするカラム\n",
    "> + left_index：leftのKeyをindexにする場合Trueを指定\n",
    "> + right_index：rightのKeyをindexにする場合Trueを指定\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - merge(how='') left right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({ 'Key': ['k0', 'k1', 'k2'],\n",
    "                    'A': ['a0', 'a1', 'a2'],\n",
    "                    'B': ['b0', 'b1', 'b2']})\n",
    "\n",
    "df2 = pd.DataFrame({ 'Key': ['k0', 'k1', 'k3'],\n",
    "                    'C': ['c0', 'c1', 'c3'],\n",
    "                    'D': ['d0', 'd1', 'd3']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df１"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1:left df2:right\n",
    "# rightの表には’k2’がありません． なので’k2’のC, DはNaNが入ります．leftの表のレコードは失われないイメージ\n",
    "df1.merge(df2, how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - merge(how='') outer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# left(df1)もright(df2)もどちらもレコードを失うことなく， 結合できるレコードは結合(以外はNaNで埋める)\n",
    "df1.merge(df2, how='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - merge(how='') inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# left(df1)もright(df2)にもある共通のレコードだけ\n",
    "df1.merge(df2, how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - merge(on='') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({ 'Key': ['k0', 'k1', 'k2'],\n",
    "                    'ID': ['aa', 'bb', 'cc'],\n",
    "                    'A': ['a0', 'a1', 'a2'],\n",
    "                    'B': ['b0', 'b1', 'b2']})\n",
    "\n",
    "df2 = pd.DataFrame({ 'Key': ['k0', 'k1', 'k3'],\n",
    "                    'ID': ['aa', 'bb', 'cc'],\n",
    "                    'C': ['c0', 'c1', 'c3'],\n",
    "                    'D': ['d0', 'd1', 'd3']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KeyカラムをKeyにして結合\n",
    "df1.merge(df2, on='Key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDカラムをKeyにして結合\n",
    "df1.merge(df2, on='ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suffixesを指定する\n",
    "df1.merge(df2, on='ID', suffixes=('_left', '_right'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - merge(left_on='', right_on='') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({ 'Key1': ['k0', 'k1', 'k2'],\n",
    "                    'A': ['a0', 'a1', 'a2'],\n",
    "                    'B': ['b0', 'b1', 'b2']})\n",
    "\n",
    "df2 = pd.DataFrame({ 'Key2': ['k0', 'k1', 'k3'],\n",
    "                    'C': ['c0', 'c1', 'c3'],\n",
    "                    'D': ['d0', 'd1', 'd3']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suffixesを指定する\n",
    "df1.merge(df2, left_on='Key1', right_on='Key2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - merge(left_index='', right_index='') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.merge(df2, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - .join()\n",
    "\n",
    "> ```memo```\n",
    ">\n",
    ">     複数のDataFrameを一気に連結できます．\n",
    ">     が...これはオススメしません．バグのもとになりやすいですし，コードが読みにくくなります．\n",
    ">     できれば一つ一つ結合することをオススメします．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({ 'Key1': ['k0', 'k1', 'k2'],\n",
    "                    'A': ['a0', 'a1', 'a2'],\n",
    "                    'B': ['b0', 'b1', 'b2']})\n",
    "\n",
    "df2 = pd.DataFrame({ 'Key2': ['k0', 'k1', 'k3'],\n",
    "                    'C': ['c0', 'c1', 'c3'],\n",
    "                    'D': ['d0', 'd1', 'd3']})\n",
    "df3 = pd.DataFrame({ 'Key3': ['k0', 'k1', 'k4'],\n",
    "                    'E': ['c0', 'c1', 'c3'],\n",
    "                    'F': ['d0', 'd1', 'd3']})\n",
    "\n",
    "df1.join([df2, df3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17: DataFrameの重要関数(apply, unique, value_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - .unique() .nunique()\n",
    "> Seriesの関数です．よく使います．\n",
    "\n",
    ">     「このカラムはどんな値を保持するんだろう？」と思いますよね？\n",
    ">     例えば「本当にPclassは1, 2, 3しか値がないのだろうか？」と思うことがあると思います． そういうときにこれらの関数を使います．\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Pclass'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Pclass'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - .value_counts()\n",
    "> それぞれの値に対していくつのレコードがあるのかをSeries形式で返してくれます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Pclass'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### | .apply（）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - (超重要).apply()\n",
    "> apply : 適用\n",
    ">\n",
    ">     基本的にDataFrameの操作はapply関数で処理していくと言っていいと思います．\n",
    ">     apply()関数を使って， DataFrameの全てのレコードに処理をして， その結果を別のカラムに格納することができます．\n",
    "\n",
    "<img src='https://datawokagaku.com/wp-content/uploads/2020/02/apply_overview-1.png' width=45%>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_age_group(age):\n",
    "    return str(age)[0] + '0s'\n",
    "\n",
    "# 実行\n",
    "get_age_group(45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({ 'name': ['john', 'Mike', 'Emily'],\n",
    "                    'age': ['23', '36', '42']})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['age'].apply(get_age_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'age_group'カラムを新たに作り，結果を代入\n",
    "df['age_group'] = df['age'].apply(get_age_group)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - lambda関数を使った.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lambda関数を変数fに代入して\n",
    "f = lambda x: str(x)[0] + '0s'\n",
    "#　試しに43を入れてみる\n",
    "f(43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['age_group'] = df['age'].apply(lambda x: str(x)[0] + '0s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - レコード全体に対し.apply()\n",
    "> 行に対してapplyする場合，axis=1を指定する必要があります．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({ 'name': ['john', 'Mike', 'Emily'],\n",
    "                    'age': ['23', '36', '42']})\n",
    "df['description'] = df.apply(lambda row: '{} is {} years old'.format(row['name'], row['age']), axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18: DataFrameのその他頻出関数(to_csv, iterrows, sort_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - df.to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Adult'] = df['Age'].apply(lambda x : x > 20)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('train_w_aduld.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../../../../train_w_adult.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = pd.read_csv('train_w_aduld.csv')\n",
    "display(df_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">    ```Unnamed:```と謎のカラムが作成される\n",
    ">     これは、indexeがそのまま保存されている。pandasで読み込むたびに生成されるのは不要\n",
    ">     オプションを付するればOK\n",
    ">\n",
    "\n",
    "> ```Python\n",
    "> df.to_csv('pass.csv', index=False)\n",
    "> ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - .iterrows()\n",
    "> DataFrameをfor文でイテレーションするときに使います．覚えにくいですが，「rows」を「iteration」するのでiter + row + s. と覚えましょう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in df.iterrows():\n",
    "    if row['Age'] > 40 and row['Pclass'] == 3 and row['Sex'] == 'male' and row[\"Survived\"] == 1:\n",
    "        print(\"{}: {} is very luck guy...!\".format(idx, row['Name']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - .sort_values()\n",
    ">     SeriesではなくDataFrameの関数であることに注意してください．\n",
    ">     デフォルトは昇順ソート(小→大)です．降順ソート(大→小)にするには ascending=False を指定します．\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values('Age', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19: DataFrameのその他頻出関数(pivot_table, xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - pivot_table()\n",
    "> データサイエンスでは殆ど用いない。頭の片隅に置いておこう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'Date':['Jan-1', 'Jan-1', 'Jan-1', 'Jan-2', 'Jan-2', 'Jan-2'], \n",
    "        'User':['Emily', 'John', 'Nick', 'Kevin', 'Emily', 'John'],\n",
    "        'Method':['Card', 'Card', 'Cash', 'Card', 'Cash', 'Cash'],\n",
    "        'Price':[100, 250, 200, 460, 200, 130]}\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.pivot_table(values='Price', index=['Date', 'User'], columns=['Method'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot = df.pivot_table(values=\"Price\", index=[\"Date\", \"User\"], columns=[\"Method\"])\n",
    "pivot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### .xs()\n",
    "> cross-section操作\n",
    ">\n",
    ">     これもあまり使いませんが，ピボットのような複数のindexをもったDataFrameを操作する際に重宝します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot.loc[\"Jan-1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot.loc[\"Card\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot.xs(\"Card\", level=\"Method\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
